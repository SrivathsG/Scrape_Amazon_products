{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Scraping Amazon product info</h2>\n",
    "<p>Have written a function to scrape all the details of a product link like:<br>->Name of product<br>->Category<br>->Description<br>->Review<br>->Rating<br>Make sure to only paste the product link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------To scrape any product individually use this function-------------------------#\n",
    "\n",
    "def scrape(url, filename):\n",
    "    ''' Parameters: Url to be scraped, filename to append the data'''\n",
    "    temp_Product_info={\n",
    "    'name': [],\n",
    "    'category': [],\n",
    "    'Description': [],\n",
    "    'Review': [],\n",
    "    'Rating': []\n",
    "   }\n",
    "    i=0\n",
    "    df_temp=pd.DataFrame(temp_Product_info)\n",
    "    proxy_temp={\n",
    "    'api_key': #Place your API key here (Scraperapi),\n",
    "    'url': url\n",
    "    }\n",
    "    print(\"\\n\\n Fetching details of the new product...\\n\")\n",
    "\n",
    "    product=requests.get('http://api.scraperapi.com', params=proxy_temp)\n",
    "    if product.status_code==200:\n",
    "        product=BeautifulSoup(product.content, 'html.parser')\n",
    "\n",
    "        prod_title=(product.find('span', id=\"productTitle\")).text \n",
    "        \n",
    "        product_desc=product.find_all('li', class_=\"a-spacing-mini\") #product description\n",
    "        product_desc[1]=product_desc[1].text\n",
    "        product_desc[0]=product_desc[0].text\n",
    "        product_description=product_desc[0]+product_desc[1]\n",
    "\n",
    "\n",
    "        reviews_link=(product.find('a', class_=\"a-link-emphasis a-text-bold\"))['href']  \n",
    "        reviews_link=\"amazon.in\"+reviews_link\n",
    "\n",
    "        #inside the review page\n",
    "\n",
    "        proxy2={\n",
    "            'api_key': 'bcf253a2f3702ac6d9df7bfdb114c216',\n",
    "            'url': reviews_link\n",
    "        }\n",
    "\n",
    "        print(\"Waiting to fetch review_page... \\n \")\n",
    "        time.sleep(45)\n",
    "        \n",
    "\n",
    "        reviews=requests.get('http://api.scraperapi.com', params=proxy2)\n",
    "\n",
    "        if reviews.status_code==200:\n",
    "            \n",
    "            print(\"Fetched\\n\")\n",
    "            reviews=BeautifulSoup(reviews.content, 'html.parser')\n",
    "            \n",
    "            reviews =reviews.find_all('div', class_=\"a-section review aok-relative\")\n",
    "            for review in reviews:\n",
    "                rating=review.find('span', class_=\"a-icon-alt\").text\n",
    "                rating=rating[0:3]\n",
    "                \n",
    "                para=review.find('span', class_=\"a-size-base review-text review-text-content\").text\n",
    "                \n",
    "\n",
    "                tuple={\n",
    "                    'name': prod_title,\n",
    "                    'category': category,\n",
    "                    'Description': product_description,\n",
    "                    'Review': para,\n",
    "                    'Rating': rating\n",
    "                }\n",
    "\n",
    "                df_temp=df_temp.append(tuple, ignore_index=True)\n",
    "\n",
    "                print(f\"finished Appending review  {i}\")\n",
    "                i+=1\n",
    "                    \n",
    "        else:\n",
    "            print(f\"Unable to fetch review page  \\n\")\n",
    "            print(reviews.status_code)\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "    else:\n",
    "        print(product.status_code)\n",
    "        print(f\"connection interrupted\")\n",
    "        \n",
    "    df_temp.to_csv(filename, mode='a') ## To write to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link=''# Paste your link here\n",
    "filename='' #paste the file name you wish to append the information to.\n",
    "scrape(link, filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
